% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inproceedings{madhani-etal-2023-aksharantar,
    title = "Aksharantar: Open {I}ndic-language Transliteration datasets and models for the Next Billion Users",
    author = "Madhani, Yash  and
      Parthan, Sushane  and
      Bedekar, Priyanka  and
      Nc, Gokul  and
      Khapra, Ruchi  and
      Kunchukuttan, Anoop  and
      Kumar, Pratyush  and
      Khapra, Mitesh",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.4",
    doi = "10.18653/v1/2023.findings-emnlp.4",
    pages = "40--57",
    abstract = "Transliteration is very important in the Indian language context due to the usage of multiple scripts and the widespread use of romanized inputs. However, few training and evaluation sets are publicly available. We introduce Aksharantar, the largest publicly available transliteration dataset for Indian languages created by mining from monolingual and parallel corpora, as well as collecting data from human annotators. The dataset contains 26 million transliteration pairs for 21 Indic languages from 3 language families using 12 scripts. Aksharantar is 21 times larger than existing datasets and is the first publicly available dataset for 7 languages and 1 language family. We also introduce a test set of 103k word pairs for 19 languages that enables a fine-grained analysis of transliteration models on native origin words, foreign words, frequent words, and rare words. Using the training set, we trained IndicXlit, a multilingual transliteration model that improves accuracy by 15{\%} on the Dakshina test set, and establishes strong baselines on the Aksharantar testset introduced in this work. The models, mining scripts, transliteration guidelines, and datasets are available at https://github.com/AI4Bharat/IndicXlit under open-source licenses.",
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@INPROCEEDINGS{Baiju2024,
  author={Baiju, Bajiyo and Manohar, Kavya and Pillai, Leena G and Sherly, Elizabeth},
  booktitle={2024 IEEE Recent Advances in Intelligent Computational Systems (RAICS)}, 
  title={{Malayalam to English Named Entity Transliteration using Attention based BiLSTM}}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Training;Attention mechanisms;Accuracy;Error analysis;Information processing;Computer architecture;Long short term memory;Transliteration;Malayalam;BiLSTM;Natural Language Processing (NLP)},
  doi={10.1109/RAICS61201.2024.10690040}}

@article{b11,
   title={Attention mechanism in neural networks: where it comes and where it goes},
   volume={34},
   ISSN={1433-3058},
   url={http://dx.doi.org/10.1007/s00521-022-07366-3},
   DOI={10.1007/s00521-022-07366-3},
   number={16},
   journal={Neural Computing and Applications},
   publisher={Springer Science and Business Media LLC},
   author={Soydaner, Derya},
   year={2022},
   month=may, pages={13371–13385} }
@inproceedings{kunchukuttan-etal-2015-brahmi,
    title = "Brahmi-Net: A transliteration and script conversion system for languages of the {I}ndian subcontinent",
    author = "Kunchukuttan, Anoop  and
      Puduppully, Ratish  and
      Bhattacharyya, Pushpak",
    editor = "Gerber, Matt  and
      Havasi, Catherine  and
      Lacatusu, Finley",
    booktitle = "Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations",
    month = jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N15-3017",
    doi = "10.3115/v1/N15-3017",
    pages = "81--85",
}
@misc{libindic,
  author       = {Santhosh Thottingal},
  year         = {2018},
  title        = {LibIndic Soundex and Transliteration Module},
  url          = {https://libindic.org/Transliteration},
  month        = {},
  lastaccessed = {December 12, 2024}
}

@misc{aksharamukha,
  author       = {Vinodh Rajan},
  year         = {2018},
  title        = {Aksharamukha script converter web application.},
  url          = {https://www.aksharamukha.com},
  month        = {},
  lastaccessed = {December 12, 2024}
}

@inproceedings{stat,
author = {Sebastian, Mary Priya and K, Sheena Kurian and Kumar, G. Santhosh},
title = {English to Malayalam Translation: A Statistical Approach},
year = {2010},
isbn = {9781450301947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858378.1858442},
doi = {10.1145/1858378.1858442},
abstract = {This paper underlines a methodology for translating text from English into the Dravidian language, Malayalam using statistical models. By using a monolingual Malayalam corpus and a bilingual English/Malayalam corpus in the training phase, the machine automatically generates Malayalam translations of English sentences. This paper also discusses a technique to improve the alignment model by incorporating the parts of speech information into the bilingual corpus. Removing the insignificant alignments from the sentence pairs by this approach has ensured better training results. Pre-processing techniques like suffix separation from the Malayalam corpus and stop word elimination from the bilingual corpus also proved to be effective in training. Various handcrafted rules designed for the suffix separation process which can be used as a guideline in implementing suffix separation in Malayalam language are also presented in this paper. The structural difference between the English Malayalam pair is resolved in the decoder by applying the order conversion rules. Experiments conducted on a sample corpus have generated reasonably good Malayalam translations and the results are verified with F measure, BLEU and WER evaluation metrics.},
booktitle = {Proceedings of the 1st Amrita ACM-W Celebration on Women in Computing in India},
articleno = {64},
numpages = {5},
keywords = {PoS tagging, English Malayalam translation, statistical machine translation, suffix separation, alignment},
location = {Coimbatore, India},
series = {A2CWiC '10}
}
@inproceedings{roark-etal-2020-processing,
    title = "Processing {South} {Asian} Languages Written in the {Latin} Script:
    the {Dakshina} Dataset",
    author = "Roark, Brian and
      Wolf-Sonkin, Lawrence and
      Kirov, Christo and
      Mielke, Sabrina J. and
      Johny, Cibu and
      Demir{\c{s}}ahin, I{\c{s}}in and
      Hall, Keith",
    booktitle = "Proceedings of The 12th Language Resources and Evaluation Conference (LREC)",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.294",
    pages = "2413--2423"
}

@inproceedings{jha-2010-tdil,
    title = "The {TDIL} Program and the {I}ndian Langauge Corpora Intitiative ({ILCI})",
    author = "Jha, Girish Nath",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/874_Paper.pdf",
    abstract = "India is considered a linguistic ocean with 4 language families and 22 scheduled national languages, and 100 un-scheduled languages reported by the 2001 census. This puts tremendous pressures on the Indian government to not only have comprehensive language policies, but also to create resources for their maintenance and development. In the age of information technology, there is a greater need to have a fine balance between allocation of resources to each language keeping in view the political compulsions, electoral potential of a linguistic community and other issues. In this connection, the government of India through various ministries and a think tank consisting of eminent linguistics and policy makers has done a commendable job despite the obvious roadblocks. This paper describes the Indian governments policies towards language development and maintenance in the age of technology through the Ministry of HRD through its various agencies and the Ministry of Communications {\&} Information Technology (MCIT) through its dedicated program called TDIL (Technology Development for Indian Languages). The paper also describes some of the recent activities of the TDIL in general and in particular, an innovative corpora project called ILCI - Indian Languages Corpora Initiative.",
}

@ARTICLE{kavya2022,
  author={Manohar, Kavya and Jayan, A. R. and Rajan, Rajeev},
  journal={IEEE Access}, 
  title={Mlphon: A Multifunctional Grapheme-Phoneme Conversion Tool Using Finite State Transducers}, 
  year={2022},
  volume={10},
  number={},
  pages={97555-97575},
  keywords={Speech recognition;Knowledge based systems;Vocabulary;Computational modeling;Text processing;Phonetics;Automatic speech recognition;Linguistics;Natural language processing;Computational Phonology;Low Resource Languages;Pronunciation Lexicon;Malayalam;Software Tool;Speech Recognition;Syllabification},
  doi={10.1109/ACCESS.2022.3204403}}

@misc{namepair,
  author       = {Santhosh Thottingal},
  year         = {2023},
  title        = {{Malayalam-English Name Pair Dataset}},
  url          = {https://huggingface.co/datasets/santhosh/english-malayalam-names},
  month        = {},
  lastaccessed = {December 12, 2024}
}

@inproceedings{kunchukuttan-etal-2021-large,
    title = "A Large-scale Evaluation of Neural Machine Transliteration for {I}ndic Languages",
    author = "Kunchukuttan, Anoop  and
      Jain, Siddharth  and
      Kejriwal, Rahul",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.303",
    doi = "10.18653/v1/2021.eacl-main.303",
    pages = "3469--3475",
    abstract = "We take up the task of large-scale evaluation of neural machine transliteration between English and Indic languages, with a focus on multilingual transliteration to utilize orthographic similarity between Indian languages. We create a corpus of 600K word pairs mined from parallel translation corpora and monolingual corpora, which is the largest transliteration corpora for Indian languages mined from public sources. We perform a detailed analysis of multilingual transliteration and propose an improved multilingual training recipe for Indic languages. We analyze various factors affecting transliteration quality like language family, transliteration direction and word origin.",
}

@inproceedings{junczys-dowmunt-etal-2018-marian,
    title = "{M}arian: Fast Neural Machine Translation in {C}++",
    author = "Junczys-Dowmunt, Marcin  and
      Grundkiewicz, Roman  and
      Dwojak, Tomasz  and
      Hoang, Hieu  and
      Heafield, Kenneth  and
      Neckermann, Tom  and
      Seide, Frank  and
      Germann, Ulrich  and
      Aji, Alham Fikri  and
      Bogoychev, Nikolay  and
      Martins, Andr{\'e} F. T.  and
      Birch, Alexandra",
    editor = "Liu, Fei  and
      Solorio, Thamar",
    booktitle = "Proceedings of {ACL} 2018, System Demonstrations",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-4020",
    doi = "10.18653/v1/P18-4020",
    pages = "116--121",
    abstract = "We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed.",
}


@article{kunchukuttan2018leveraging,
  title={Leveraging orthographic similarity for multilingual neural transliteration},
  author={Kunchukuttan, Anoop and Khapra, Mitesh and Singh, Gurneet and Bhattacharyya, Pushpak},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={303--316},
    url={https://doi.org/10.1162/tacl_a_00022},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{kirov2024context,
  title={{Context-aware Transliteration of Romanized South Asian Languages}},
  author={Kirov, Christo and Johny, Cibu and Katanova, Anna and Gutkin, Alexander and Roark, Brian},
  journal={Computational Linguistics},
  pages={1--60},
    url ={https://doi.org/10.1162/coli_a_00510},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@misc{iso,
  author       = {ISO 15919:2001 Transliteration},
  year         = {2001},
  title        = {{Transliteration of Devanagari and related Indic scripts into Latin characters}},
  url          = {https://www.iso.org/standard/28333.html},
  month        = {},
  lastaccessed = {December 12, 2024}
}

@inproceedings{parlikar2016festvox,
  title     = {The {F}estvox {I}ndic frontend for grapheme to phoneme conversion},
  author    = {Parlikar, Alok and Sitaram, Sunayana and Wilkinson, Andrew and Black, Alan W},
  booktitle = {WILDRE: Workshop on {I}ndian Language Data-Resources and Evaluation},
  year      = {2016}
}

@inproceedings{baby2016resources,
  title        = {Resources for {I}ndian languages},
  author       = {Baby, Arun and Thomas, Anju Leela and Nishanthi, NL and TTS Consortium and others},
  booktitle    = {Proceedings of Text, Speech and Dialogue},
  year         = {2016},
  organization = {CBBLR Workshop}
}

@inproceedings{manghat2020malayalam,
  author    = {Sreeja Manghat and Sreeram Manghat and Tanja Schultz},
  title     = {{Malayalam-English Code-Switched: Grapheme to Phoneme System}},
  year      = 2020,
  booktitle = {Proc. Interspeech 2020},
  pages     = {4133--4137},
  doi       = {10.21437/Interspeech.2020-1936},
  url       = {http://doi.org/10.21437/Interspeech.2020-1936}
}

@InProceedings{Priyamvada_2021,
author="Priyamvada, R.
and Govind, D.
and Menon, Vijay Krishna
and Premjith, B.
and Soman, K. P.",
editor="Satapathy, Suresh Chandra
and Peer, Peter
and Tang, Jinshan
and Bhateja, Vikrant
and Ghosh, Anumoy",
title="Grapheme to Phoneme Conversion for Malayalam Speech Using Encoder-Decoder Architecture",
booktitle="Intelligent Data Engineering and Analytics",
year="2022",
publisher="Springer Nature Singapore",
address="Singapore",
pages="41--49"
}

@article{james2024advocating,
  title={Advocating Character Error Rate for Multilingual ASR Evaluation},
  author={James, Jesin and Gopinath, Deepa P and others},
  journal={arXiv preprint arXiv:2410.07400},
  year={2024}
}
